{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c31a870",
   "metadata": {},
   "source": [
    "# Training From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906be20",
   "metadata": {},
   "source": [
    "## Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35181b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "imdb_df = pd.read_csv(\"IMDB-Dataset.csv\")\n",
    "reviews = imdb_df.review.to_string(index=None) \n",
    "with open(\"corpus.txt\", \"w\") as f: \n",
    "    f.writelines(reviews) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84012656",
   "metadata": {},
   "source": [
    "## Training the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28536364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer() \n",
    "bert_wordpiece_tokenizer.train(\"corpus.txt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda89246",
   "metadata": {},
   "source": [
    "To see the tokenized words along with their index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4aa8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_wordpiece_tokenizer.get_vocab() # for all items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7286268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stow', 11786), ('carrie', 11020), ('disbel', 8959), ('astronauts', 17309)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bert_wordpiece_tokenizer.get_vocab().items())[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38464674",
   "metadata": {},
   "source": [
    "### Saving/Loading the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9665a9",
   "metadata": {},
   "source": [
    "We have to save the tokenizer so we can reuse it later for training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec3b3802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tokenizer/vocab.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To save\n",
    "!mkdir tokenizer\n",
    "bert_wordpiece_tokenizer.save_model(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cccdb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer() \n",
    "mytokenizer = BertWordPieceTokenizer.from_file(\"tokenizer/vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309e679",
   "metadata": {},
   "source": [
    "### Using the tokenizer (just for testing purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc79283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = mytokenizer.encode(\"Oh it works just fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e5122d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'oh', 'it', 'works', 'just', 'fine', '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70dd94",
   "metadata": {},
   "source": [
    "Similarly, for unknown words, it will be broken into subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6ce8609",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = mytokenizer.encode(\"ohoh i thougt it might be workingg well\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbdd5707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'oh', '##o', '##h', 'i', 'thoug', '##t', 'it', 'might', 'be', 'working', '##g', 'well', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentence.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4388977",
   "metadata": {},
   "source": [
    "### Saving Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e53cbc",
   "metadata": {},
   "source": [
    "Omitting this step will give a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained('distilroberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b68248",
   "metadata": {},
   "source": [
    "## Tokenization (i.e. processing input for model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd50f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file tokenizer/config.json not found\n",
      "file tokenizer/config.json not found\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast \n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc39cb",
   "metadata": {},
   "source": [
    "## Creating the Dataset for MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da58b054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/data/datasets/language_modeling.py:120: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# OLD WAY\n",
    "from transformers import LineByLineTextDataset \n",
    "dataset_old = LineByLineTextDataset(tokenizer=tokenizer, file_path=\"corpus.txt\", block_size=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "016ba2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50022"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0476c934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   2, 2129,  136, 2237,  148,  195,  146,  508, 3868,  394,  169,   18,\n",
       "           18,   18,    3])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_old[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "570a86ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] phil the alien is one of those quirky films wh . . . [SEP]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer.convert_ids_to_tokens(dataset_old[10]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89a15bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1fabf752ad1491af\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-1fabf752ad1491af/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20f1f9742234ea0bb469d3dad7fa0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NEW WAY\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('text', data_files='corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d2a8678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 50028\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1ee9031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Phil the Alien is one of those quirky films wh...'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2d94b",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e687652e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01157dedda1148349d23b4c74e4fd9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map( lambda d: tokenizer(d['text']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa465288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'text', 'token_type_ids'],\n",
       "        num_rows: 50028\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca9edba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "074749dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101,\n",
      "               6316,\n",
      "               1996,\n",
      "               7344,\n",
      "               2003,\n",
      "               2028,\n",
      "               1997,\n",
      "               2216,\n",
      "               21864,\n",
      "               15952,\n",
      "               3152,\n",
      "               1059,\n",
      "               2232,\n",
      "               1012,\n",
      "               1012,\n",
      "               1012,\n",
      "               102],\n",
      " 'text': ' Phil the Alien is one of those quirky films wh...',\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenized_dataset[\"train\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3d8d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54562b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101,\n",
      "               6316,\n",
      "               1996,\n",
      "               7344,\n",
      "               2003,\n",
      "               2028,\n",
      "               1997,\n",
      "               2216,\n",
      "               21864,\n",
      "               15952,\n",
      "               3152,\n",
      "               1059,\n",
      "               2232,\n",
      "               1012,\n",
      "               1012,\n",
      "               1012,\n",
      "               102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenized_dataset[\"train\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a5dd612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cpu training and demo purposes\n",
    "# tokenized_train_dataset_1k = tokenized_dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "tokenized_train_dataset_1k = tokenized_dataset['train'].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ee4ebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'token_type_ids'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset_1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0da6e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  6316,\n",
       "  1996,\n",
       "  7344,\n",
       "  2003,\n",
       "  2028,\n",
       "  1997,\n",
       "  2216,\n",
       "  21864,\n",
       "  15952,\n",
       "  3152,\n",
       "  1059,\n",
       "  2232,\n",
       "  1012,\n",
       "  1012,\n",
       "  1012,\n",
       "  102],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset_1k[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a42c1a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'input_ids': tensor([  101,  6316,  1996,  7344,  2003,  2028,  1997,  2216, 21864, 15952,\n",
       "          3152,  1059,  2232,  1012,  1012,  1012,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset_1k.set_format(\"torch\")\n",
    "tokenized_train_dataset_1k[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d387d9",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d183eb7",
   "metadata": {},
   "source": [
    "This is like a pre-processing function for the input. In this case, it will mask the input with a probability of 15%.\n",
    "\n",
    "Note that any preprocessing will be done on the fly as the data is passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b28d8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling \n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5618e6",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50ae08ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments \n",
    "training_args = TrainingArguments(output_dir=\"BERT\",\n",
    "                                  overwrite_output_dir=True,\n",
    "                                  num_train_epochs=1,\n",
    "                                  per_device_train_batch_size=128) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896db55",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e41539",
   "metadata": {},
   "source": [
    "Here, we will provide the configurations for the model. We will inherit from the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa1d0b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.11.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig \n",
    "BertConfig() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33959b6b",
   "metadata": {},
   "source": [
    "To change configuration, set the appropraite values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3a6d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny_bert_config = BertConfig(max_position_embeddings=512, hidden_size=128, num_attention_heads=2, num_hidden_layers=2, intermediate_size=512) \n",
    "# tiny_bert_config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11ba422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny_bert = BertForMaskedLM(tiny_bert_config) \n",
    "# trainer = Trainer(model=tiny_bert, args=training_args, data_collator=data_collator, train_dataset=dataset) \n",
    "# trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae019d",
   "metadata": {},
   "source": [
    "But we will use the default settings for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0a6c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM \n",
    "bert = BertForMaskedLM(BertConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2213f",
   "metadata": {},
   "source": [
    "## Trainer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f43a31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer \n",
    "trainer = Trainer(model=bert,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=tokenized_train_dataset_1k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15b11308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 02:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8, training_loss=8.57533073425293, metrics={'train_runtime': 154.6716, 'train_samples_per_second': 6.465, 'train_steps_per_second': 0.052, 'total_flos': 12284262122400.0, 'train_loss': 8.57533073425293, 'epoch': 1.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64e0f672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MyBERT\n",
      "Configuration saved in MyBERT/config.json\n",
      "Model weights saved in MyBERT/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"MyBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a47fbe",
   "metadata": {},
   "source": [
    "# Training for NLP Tasks - Without Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1334c8",
   "metadata": {},
   "source": [
    "Now that the model has been trained from scratch, we can use it to train for other task. Let's train it for a Sentiment Classification task on the same IMDB dataset, but this time the output will be either a 0 (negative) or 1 (positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df829266",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ee0637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../models/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizerFast \n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\") \n",
    "# bert.layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6fb4c",
   "metadata": {},
   "source": [
    "## Feeding a Single Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d50e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.batch_encode_plus([\"hello how is it going with you\",\"lets test it\"], return_tensors=\"pt\", max_length=256, truncation=True, padding='max_length') \n",
    "# tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c05096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 1.0047e-01,  6.7703e-02, -8.3360e-02,  ..., -4.9330e-01,\n",
       "           1.1654e-01,  2.2665e-01],\n",
       "         [ 3.2362e-01,  3.7072e-01,  6.1469e-01,  ..., -6.2727e-01,\n",
       "           3.7908e-01,  7.0531e-02],\n",
       "         [ 1.9953e-01, -8.7551e-01, -6.4786e-02,  ..., -1.2808e-02,\n",
       "           3.0765e-01, -2.0732e-02],\n",
       "         ...,\n",
       "         [-6.5330e-02,  1.1905e-01,  5.7685e-01,  ..., -2.9546e-01,\n",
       "           2.4974e-02,  1.1396e-01],\n",
       "         [-2.6472e-01, -7.8638e-02,  5.4728e-01,  ..., -1.3752e-01,\n",
       "          -5.9469e-02, -5.1793e-02],\n",
       "         [-2.4496e-01, -1.1480e-01,  5.9217e-01,  ..., -1.5688e-01,\n",
       "          -3.3976e-02, -8.4614e-02]],\n",
       "\n",
       "        [[ 2.9457e-02,  2.3087e-01,  2.9265e-01,  ..., -1.3042e-01,\n",
       "           1.8966e-01,  4.6843e-01],\n",
       "         [ 1.7052e+00,  6.9136e-01,  7.3151e-01,  ...,  2.8930e-01,\n",
       "           5.3676e-01, -1.5455e-01],\n",
       "         [ 1.0460e-01,  9.6368e-02,  6.9966e-02,  ..., -4.1592e-01,\n",
       "          -1.1899e-01, -6.7224e-01],\n",
       "         ...,\n",
       "         [ 8.0091e-01,  2.3898e-01,  4.1549e-01,  ...,  3.9053e-02,\n",
       "           2.3437e-01,  1.2228e-01],\n",
       "         [ 2.6086e-01,  4.4327e-02,  3.6365e-01,  ..., -7.5365e-04,\n",
       "           3.8462e-02, -2.1421e-01],\n",
       "         [-2.3011e-01, -4.9839e-01, -1.2649e-02,  ...,  4.4987e-01,\n",
       "           6.1602e-02, -2.6136e-01]]], grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.9205, -0.3714, -0.6051,  ..., -0.4474, -0.6435,  0.9423],\n",
       "        [-0.8854, -0.2655,  0.2101,  ...,  0.1724, -0.6403,  0.8888]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(**tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10883ccd",
   "metadata": {},
   "source": [
    "Our model has two outputs: `last_hidden_state` and `pooler_output`.  \n",
    "\n",
    "The `last_hidden_state` provides all token embeddings from BERT with additional [CLS] and [SEP] tokens at the start and end, respectively.\n",
    "    \n",
    "We will add layers to these outputs to complete our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fade9788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(**tokenized_text)[1].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5617d017",
   "metadata": {},
   "source": [
    "## Creating the Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5076cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4926b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "       \n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "        self.out = nn.Linear(bert.config.hidden_size, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        \n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "#         output = self.out(outputs.pooler_output)\n",
    "        output = self.out(outputs.last_hidden_state[:,1,:])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daaeb679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../models/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6925c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, [(768, 1), (768, 1), (768, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1eaac2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04e31ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109483778"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of parameters\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d42365e",
   "metadata": {},
   "source": [
    "## Feeding a Single Input to the (Complete) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ebcc031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.batch_encode_plus([\"hello how is it going with you\",\"hello how is it going with you\"], return_tensors=\"pt\", max_length=256, truncation=True, padding='max_length') \n",
    "# tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d4380db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256]), torch.Size([2, 256]), torch.Size([2, 256]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[\"input_ids\"].size(), tokenized_text[\"attention_mask\"].size(), tokenized_text[\"token_type_ids\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b9d63f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(tokenized_text[\"input_ids\"],tokenized_text[\"attention_mask\"],tokenized_text[\"token_type_ids\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7aae5d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f836478c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6137, -0.1417],\n",
       "        [-0.6137, -0.1417]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b99f1",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d681930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizerFast \n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a406c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# imdb_df = pd.read_csv(\"IMDB-Dataset.csv\") \n",
    "imdb_df = pd.read_csv(\"IMDB-Dataset.csv\").head(100)\n",
    "reviews = list(imdb_df.review) \n",
    "tokenized_reviews = tokenizer.batch_encode_plus(reviews, return_tensors=\"pt\", max_length=256, truncation=True, padding='max_length') \n",
    "\n",
    "import numpy as np \n",
    "train_split = int(0.8 * len(tokenized_reviews[\"attention_mask\"])) \n",
    "train_tokens = tokenized_reviews[\"input_ids\"][:train_split] \n",
    "val_tokens = tokenized_reviews[\"input_ids\"][train_split:] \n",
    "train_masks = tokenized_reviews[\"attention_mask\"][:train_split] \n",
    "val_masks = tokenized_reviews[\"attention_mask\"][train_split:]\n",
    "train_ids = tokenized_reviews[\"token_type_ids\"][:train_split] \n",
    "val_ids = tokenized_reviews[\"token_type_ids\"][train_split:] \n",
    "sentiments = list(imdb_df.sentiment) \n",
    "labels = np.array([1 if sentiment == \"positive\" else 0 for sentiment in sentiments]) \n",
    "train_labels = torch.tensor(labels[:train_split])\n",
    "val_labels = torch.tensor(labels[train_split:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db1baabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feca4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2670abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "# train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "# train_y = torch.tensor(train_pd['label'], dtype=torch.long)\n",
    "\n",
    "# test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "# test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "# test_y = torch.tensor(test_pd['label'], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a4e1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d9fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_tokens, train_masks, train_ids, train_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab2dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TensorDataset(val_tokens, val_masks, val_ids, val_labels)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_loader = DataLoader(val_dataset, sampler=val_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c4d93",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e1f6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import tableprint as tp\n",
    "import torchmetrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b92990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_TL(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super(Model_TL, self).__init__()\n",
    "        self.model = model\n",
    "        self.avg_train_loss = 0.\n",
    "        self.avg_valid_loss = 0.\n",
    "        self.table_context = None\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.start_time = 0\n",
    "        self.end_time = 0\n",
    "        self.epoch_mins = 0\n",
    "        self.epoch_secs = 0\n",
    "        self.table_context = None\n",
    "        self.train_accm = torchmetrics.Accuracy()\n",
    "        self.valid_accm = torchmetrics.Accuracy()\n",
    "        self.train_acc = 0.\n",
    "        self.valid_acc = 0.\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=0.0005)\n",
    "        return optim\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, masks, type_ids, labels = batch\n",
    "        output = self.model(input_ids, masks, type_ids)\n",
    "        _, predictions = torch.max(output, 1)\n",
    "        acc_train = self.train_accm(predictions, labels)\n",
    "        loss = self.loss_fn(output, labels)\n",
    "        return {\"loss\": loss, \"p\": predictions, \"y\": labels}\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, masks, type_ids, labels = batch\n",
    "        output = self.model(input_ids, masks, type_ids)\n",
    "        _, predictions = torch.max(output, 1)\n",
    "        acc_train = self.valid_accm(predictions, labels)\n",
    "        loss_valid = self.loss_fn(output, labels)\n",
    "        return {\"loss\": loss_valid, \"p\": predictions, \"y\": labels}\n",
    "\n",
    "\n",
    "    def on_train_epoch_start(self) :\n",
    "        self.start_time = time.time()\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if self.trainer.sanity_checking:\n",
    "            return\n",
    "        \n",
    "        self.avg_valid_loss = torch.stack([x['loss'] for x in outputs]).mean().item()\n",
    "        self.valid_acc = (self.valid_accm.compute() * 100).item()\n",
    "        self.valid_accm.reset()\n",
    "        self.log(\"epoch_num\", int(self.current_epoch+1), on_step=False, on_epoch=True, prog_bar=False, logger=False)\n",
    "        self.log(\"val_loss\", self.avg_valid_loss, on_step=False, on_epoch=True, prog_bar=False, logger=False)\n",
    "        self.log(\"val_acc\", self.valid_acc, on_step=False, on_epoch=True, prog_bar=False, logger=False)\n",
    "        \n",
    "#         if self.current_epoch == self.trainer.max_epochs - 1:\n",
    "#             y = torch.cat([x['y'] for x in outputs])\n",
    "#             p = torch.cat([x['p'] for x in outputs])\n",
    "          \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.avg_train_loss = torch.stack([x['loss'] for x in outputs]).mean().item()\n",
    "        self.train_acc = (self.train_accm.compute() * 100).item()\n",
    "        self.train_accm.reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.end_time = time.time()\n",
    "        time_int = self.format_time(self.start_time, self.end_time)\n",
    "    \n",
    "        metrics = {'epoch': self.current_epoch+1, 'Train Acc': self.train_acc, 'Train Loss': self.avg_train_loss,  'Valid Acc': self.valid_acc, 'Valid Loss': self.avg_valid_loss}\n",
    "        if self.table_context is None:\n",
    "            self.table_context = tp.TableContext(headers=['epoch', 'Train Acc', 'Train Loss', 'Valid Acc', 'Valid Loss', 'Time'])\n",
    "            self.table_context.__enter__()\n",
    "        self.table_context([self.current_epoch+1, self.train_acc, self.avg_train_loss, self.valid_acc, self.avg_valid_loss, time_int])\n",
    "        self.logger.log_metrics(metrics)\n",
    "\n",
    "        if self.current_epoch == self.trainer.max_epochs - 1:\n",
    "            self.table_context.__exit__()\n",
    "\n",
    "    \n",
    "    def format_time(self, start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_rounded = int(round((elapsed_time)))\n",
    "        return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba159621",
   "metadata": {},
   "outputs": [],
   "source": [
    "plmodel = Model_TL(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d14fe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='./',\n",
    "    filename='model',\n",
    "    mode='max'\n",
    ")\n",
    "csvlogger = CSVLogger('csv_logs', name='Ch3', version=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa71330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loggers/csv_logs.py:57: UserWarning: Experiment logs directory csv_logs/Ch3/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | model      | Model            | 109 M \n",
      "1 | loss_fn    | CrossEntropyLoss | 0     \n",
      "2 | train_accm | Accuracy         | 0     \n",
      "3 | valid_accm | Accuracy         | 0     \n",
      "------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b356b03f8434af3b86e630cdfa65a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1, num_sanity_val_steps=0, logger=csvlogger, gpus=0, callbacks=[checkpoint_callback], log_every_n_steps=1)\n",
    "trainer.fit(plmodel, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92802a4f",
   "metadata": {},
   "source": [
    "# Other autoencoding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b0830",
   "metadata": {},
   "source": [
    "## BERT-BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1bb293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109.48224 million parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "bert_base= BertConfig()\n",
    "model = BertModel(bert_base)\n",
    "print(f\"{model.num_parameters() /(10**6)} million parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8e4c6",
   "metadata": {},
   "source": [
    "## Albert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eefaa8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.683584 million parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertConfig, AlbertModel\n",
    "albert_base = AlbertConfig(\n",
    "     hidden_size=768,\n",
    "     num_attention_heads=12,\n",
    "     intermediate_size=3072,\n",
    " )\n",
    "model = AlbertModel(albert_base)\n",
    "print(f\"{model.num_parameters() /(10**6)} million parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e3758",
   "metadata": {},
   "source": [
    "## BERT-LARGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0fb136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335.141888 million parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "bert_large= BertConfig(hidden_size=1024, \n",
    "                      num_hidden_layers=24 ,\n",
    "          num_attention_heads=16,\n",
    "          intermediate_size=4096\n",
    "     )\n",
    "model = BertModel(bert_large)\n",
    "print(f\"{model.num_parameters() /(10**6)} million parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e7343c",
   "metadata": {},
   "source": [
    "## ALBERT-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f36037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222.595584 million parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertConfig, AlbertModel\n",
    "albert_xxlarge= AlbertConfig()\n",
    "model = AlbertModel(albert_xxlarge)\n",
    "print(f\"{model.num_parameters() /(10**6)} million parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a2d69",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1356feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109.48224 million parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaModel\n",
    "conf= RobertaConfig()\n",
    "model = RobertaModel(conf)\n",
    "print(f\"{model.num_parameters() /(10**6)} million parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093025e9",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac41c008",
   "metadata": {},
   "source": [
    "## Loading a Turkish Pre-trained Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f5e948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdbf7715e654cdd8ceec1d4568712a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b863eed1504c6693523c822600cb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc598b7ce88f4229ac0da59d8595388b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC size is: 32000\n",
      "The model is <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizerTUR = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-uncased\",)\n",
    "print(f\"VOC size is: {tokenizerTUR.vocab_size}\")\n",
    "print(f\"The model is {type(tokenizerTUR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1b5c0",
   "metadata": {},
   "source": [
    "## Loading an English Pre-trained Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba4ed911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc4298ae24949aebf095bba86022058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d51f58a58d424492398b1191d60983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a5f1309aba45669173265c82d2384a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36abba4c2dd45949cc3db83a7873204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC size is: 30522\n",
      "The model is <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizerEN = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(f\"VOC size is: {tokenizerEN.vocab_size}\")\n",
    "print(f\"The model is {type(tokenizerEN)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf43e3",
   "metadata": {},
   "source": [
    "## Tokenizing Word in different language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d16dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is in Turkish Model ? False\n",
      "is in English Model ? True\n"
     ]
    }
   ],
   "source": [
    "word_en=\"telecommunications\"\n",
    "print(f\"is in Turkish Model ? {word_en in tokenizerTUR.vocab}\")\n",
    "print(f\"is in English Model ? {word_en in tokenizerEN.vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb6ced15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tel', '##eco', '##mm', '##un', '##ica', '##tions']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=tokenizerTUR.tokenize(word_en)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3efb677b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['telecommunications']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens= tokenizerEN.tokenize(word_en)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e503a6d",
   "metadata": {},
   "source": [
    "# The tokenizers library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec58541",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "- Modeling\n",
    "- Normalizer\n",
    "- PreTokenizer\n",
    "- Post-Processor\n",
    "- Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa479ab4",
   "metadata": {},
   "source": [
    "## Obtaining Data for subsequent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1b3d6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import gutenberg \n",
    "nltk.download('gutenberg') \n",
    "nltk.download('punkt') \n",
    "plays=['shakespeare-macbeth.txt','shakespeare-hamlet.txt','shakespeare-caesar.txt']\n",
    "shakespeare=[\" \".join(s) for ply in plays for s in gutenberg.sents(ply)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e5c41",
   "metadata": {},
   "source": [
    "## Training BPE from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8186c2",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d86e0e",
   "metadata": {},
   "source": [
    "We have to chose the type of Model we want to train. Examples include WordLevel, BPE, WordPiece, and Unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8748a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df7ebd",
   "metadata": {},
   "source": [
    "### Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b96ab2",
   "metadata": {},
   "source": [
    "It is reponsible for pre-processing the input string in order to normalize it as relevant for a given use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1670c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import (Sequence, Lowercase, NFD, StripAccents)\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595cf05",
   "metadata": {},
   "source": [
    "### PreTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866e97b",
   "metadata": {},
   "source": [
    "This is responsible for splitting the input according to a set of rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bf21118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda71eac",
   "metadata": {},
   "source": [
    "### Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea222b29",
   "metadata": {},
   "source": [
    "This defines how to add special tokens to our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7839c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "special_tokens= [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "temp_proc= TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b14fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor=temp_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54927d",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b414b7e",
   "metadata": {},
   "source": [
    "This lets the tokenizer convert the token IDs back to readable text by processing the special characters or identifiers used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feacfc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.decoders import BPEDecoder\n",
    "tokenizer.decoder = BPEDecoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9dd4a6",
   "metadata": {},
   "source": [
    "### Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5cdbee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Trained vocab size: 5000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(vocab_size=5000, special_tokens= special_tokens)\n",
    "tokenizer.train_from_iterator(shakespeare, trainer=trainer)\n",
    "print(f\"Trained vocab size: {tokenizer.get_vocab_size()}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d462d866",
   "metadata": {},
   "source": [
    "## Using the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "399da3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ['[CLS]', 'is', 'this', 'a', 'dagger', 'which', 'i', 'see', 'before', 'me', ',', 'the', 'hand', 'le', 'toward', 'my', 'hand', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sen= \"Is this a dagger which I see before me, the handle toward my hand?\"\n",
    "sen_enc=tokenizer.encode(sen)\n",
    "print(f\"Output: {format(sen_enc.tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e14d8855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ['[CLS]', 'macbeth', 'and', 'hu', 'gg', 'ing', 'face', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sen_enc2=tokenizer.encode(\"Macbeth and Hugging Face\")\n",
    "print(f\"Output: {format(sen_enc2.tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a007579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ['[CLS]', 'i', 'like', 'hu', 'gg', 'ing', 'face', '!', '[SEP]', 'he', 'likes', 'macbeth', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "two_enc=tokenizer.encode(\"I like Hugging Face!\",\"He likes Macbeth!\")\n",
    "print(f\"Output: {format(two_enc.tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36aa58",
   "metadata": {},
   "source": [
    "## Saving the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e53d01",
   "metadata": {},
   "source": [
    "### Just the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c689f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir token_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "831c4042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token_trained/vocab.json', 'token_trained/merges.txt']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.save('token_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "160976bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "4948 ./token_trained/merges.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./token_trained/merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2549b386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "#version: 0.2 - Trained by `huggingface/tokenizers`\n",
      "t h\n",
      "o u\n",
      "a n\n",
      "th e\n",
      "r e\n"
     ]
    }
   ],
   "source": [
    "!head -6 ./token_trained/merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3772a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "ch ance\n",
      "si g\n",
      "your s\n",
      "ti a\n",
      "po int\n"
     ]
    }
   ],
   "source": [
    "!head -1000 ./token_trained/merges.txt| tail -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b5115a",
   "metadata": {},
   "source": [
    "### Entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd7f1ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir token_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22a033ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ['[CLS]', 'i', 'like', 'hu', 'gg', 'ing', 'face', 'and', 'macbeth', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save(\"token_pipeline/MyBPETokenizer.json\")\n",
    "tokenizerFromFile=Tokenizer.from_file(\"token_pipeline/MyBPETokenizer.json\")\n",
    "sen_enc3 = tokenizerFromFile.encode(\"I like HuggingFace and Macbeth\")\n",
    "print(f\"Output: {format(sen_enc3.tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c47c26",
   "metadata": {},
   "source": [
    "## Training WordPiece from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b19c2",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adfca21",
   "metadata": {},
   "source": [
    "We have to chose the type of Model we want to train. Examples include WordLevel, BPE, WordPiece, and Unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd614c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "tokenizer = Tokenizer(WordPiece())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ceac64",
   "metadata": {},
   "source": [
    "### Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ecdee",
   "metadata": {},
   "source": [
    "It is reponsible for pre-processing the input string in order to normalize it as relevant for a given use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c4efc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import BertNormalizer \n",
    "tokenizer.normalizer=BertNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf27f2",
   "metadata": {},
   "source": [
    "### PreTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011b454",
   "metadata": {},
   "source": [
    "This is responsible for splitting the input according to a set of rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ae8ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcca37",
   "metadata": {},
   "source": [
    "### Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4e4f7",
   "metadata": {},
   "source": [
    "This defines how to add special tokens to our sentences. (Not needed here!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20bf745",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59a262",
   "metadata": {},
   "source": [
    "This lets the tokenizer convert the token IDs back to readable text by processing the special characters or identifiers used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7715afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
    "tokenizer.decoder= WordPieceDecoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a881b",
   "metadata": {},
   "source": [
    "### Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d6372f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['is', 'this', 'a', 'dagger', 'which', 'i', 'see', 'before', 'me', ',', 'the', 'hand', '##le', 'toward', 'my', 'hand', '?']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "trainer = WordPieceTrainer(vocab_size=5000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.train_from_iterator(shakespeare, trainer=trainer)\n",
    "output = tokenizer.encode(sen)\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d440230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is this a dagger which i see before me , the hand ##le toward my hand ?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "301856cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[UNK]', 'macbeth', '!']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"KralsÄ±n aslansÄ±n Macbeth!\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a1eac",
   "metadata": {},
   "source": [
    "# Untrained ready-to-use Tokenizer Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2f14a",
   "metadata": {},
   "source": [
    "Only the above steps have been associated with these pipelines. You have to train these pipelines! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04841b03",
   "metadata": {},
   "source": [
    "* CharBPETokenizer: The original BPE\n",
    "* ByteLevelBPETokenizer: The byte level version of the BPE\n",
    "* SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
    "* BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9981eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                            CharBPETokenizer,\n",
    "                            SentencePieceBPETokenizer,\n",
    "                            BertWordPieceTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d2c0a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.normalizers.NFKC object at 0x7f4530d96970>\n",
      "<tokenizers.pre_tokenizers.Metaspace object at 0x7f4530d96970>\n",
      "<tokenizers.decoders.Metaspace object at 0x7f4530e02960>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tokenizer= SentencePieceBPETokenizer()\n",
    "print(tokenizer.normalizer)\n",
    "print(tokenizer.pre_tokenizer)\n",
    "print(tokenizer.decoder)\n",
    "print(tokenizer.post_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c24090a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.normalizers.BertNormalizer object at 0x7f4530d964b0>\n",
      "<tokenizers.pre_tokenizers.BertPreTokenizer object at 0x7f4530d964b0>\n",
      "<tokenizers.decoders.WordPiece object at 0x7f4530e02390>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tokenizer= BertWordPieceTokenizer()\n",
    "print(tokenizer.normalizer)\n",
    "print(tokenizer.pre_tokenizer)\n",
    "print(tokenizer.decoder)\n",
    "print(tokenizer.post_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448cc0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54567dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
